{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gzip\n",
    "import os \n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import urllib\n",
    "#from six.moves import xrange\n",
    "import tensorflow  as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_url=\"https://storage.googleapis.com/cvdf-datasets/mnist/\"\n",
    "work_directory=\"data_mnist\"\n",
    "image_size=28\n",
    "num_channel=1\n",
    "pixel_depth=255\n",
    "num_labels=10\n",
    "validation_size=5000\n",
    "seed=423\n",
    "batch_size=128\n",
    "num_epochs=10\n",
    "eval_batch_size=64\n",
    "eval_frequency=10\n",
    "n_classes=10\n",
    "\n",
    "\n",
    "FLAGS=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_type():\n",
    "    if FLAGS.use_fps16:\n",
    "        return tf.float16\n",
    "    else:\n",
    "        return tf.float32\n",
    "    \n",
    "def maybe_download(filename):\n",
    "    if not tf.gfile.Exists(work_directory):\n",
    "        tf.gfile.MakeDirs(work_directory)\n",
    "    filepath=os.path.join(work_directory,filename)\n",
    "    if not tf.gfile.Exists(filepath):\n",
    "        filepath,_=urllib.request.urlretrieve(source_url+filename,filepath)\n",
    "        with tf.gfile.GFile(filepath) as f:\n",
    "            size=f.size()\n",
    "        print(\"successfully download\",filename,size,\"bytes.\")\n",
    "    return filepath\n",
    "\n",
    "def extract_data(filename,num_images):\n",
    "    \n",
    "    print(\"extracting\",filename)\n",
    "    with gzip.open(filename) as byt:\n",
    "        byt.read(16)\n",
    "        buf=byt.read(image_size*image_size*num_images*num_channel)\n",
    "        data=np.frombuffer(buf,dtype=np.uint8).astype(np.float32)\n",
    "        data=(data-(pixel_depth/2.0))/pixel_depth\n",
    "        data=data.reshape(num_images,image_size,image_size,num_channel)\n",
    "    return data\n",
    "    \n",
    "def extract_label(filename,num_images):\n",
    "    print(\"extracting\",filename)\n",
    "    with gzip.open(filename) as byt:\n",
    "        byt.read(8)\n",
    "        buf=byt.read(1*num_images)\n",
    "        labels=np.frombuffer(buf,dtype=np.uint8).astype(np.int64)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting data_mnist/train-images-idx3-ubyte.gz\n",
      "extracting data_mnist/train-labels-idx1-ubyte.gz\n",
      "extracting data_mnist/t10k-images-idx3-ubyte.gz\n",
      "extracting data_mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# get data\n",
    "train_data_filename=maybe_download(\"train-images-idx3-ubyte.gz\")\n",
    "train_labels_filename=maybe_download(\"train-labels-idx1-ubyte.gz\")\n",
    "test_data_filename=maybe_download(\"t10k-images-idx3-ubyte.gz\")\n",
    "test_labels_filename=maybe_download(\"t10k-labels-idx1-ubyte.gz\")\n",
    "\n",
    "# extract data\n",
    "train_data=extract_data(train_data_filename,60000)\n",
    "train_labels=extract_label(train_labels_filename,60000)\n",
    "test_data=extract_data(test_data_filename,10000)\n",
    "test_labels=extract_label(test_labels_filename,10000)\n",
    "\n",
    "# generate validation data\n",
    "validation_data=train_data[:validation_size]\n",
    "validation_labels=train_labels[:validation_size]\n",
    "train_data=train_data[validation_size:]\n",
    "train_labels=train_labels[validation_size:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "train=True\n",
    "train_size=train_labels.shape[0]\n",
    "epochs=num_epochs\n",
    "epsilon = 1e-3\n",
    "#phase=True\n",
    "#\n",
    "x=tf.placeholder(dtype=tf.float32,shape=[None,image_size,image_size,num_channel],\n",
    "                name=\"x\")\n",
    "y=tf.placeholder(dtype=tf.int32,shape=[None],name=\"y\")\n",
    "phase = tf.placeholder(tf.bool, name='phase')\n",
    "\n",
    "\n",
    "#CONV LAYER_1\n",
    "conv1_weights=tf.Variable(tf.truncated_normal([5,5,num_channel,32],stddev=0.5),name=\"conv1_w\")\n",
    "conv1_bias=tf.Variable(tf.truncated_normal([32],0,0.1),name=\"conv1_b\")\n",
    "conv1=tf.nn.bias_add(tf.nn.conv2d(x,conv1_weights,strides=[1,1,1,1],padding=\"SAME\",name=\"conv1\"),conv1_bias)\n",
    "conv1_bn=tf.contrib.layers.batch_norm(conv1,center=True,scale=True,is_training=phase)\n",
    "relu1=tf.nn.relu(conv1_bn,name=\"relu1\")\n",
    "pool1=tf.nn.max_pool(relu1,ksize=(1,2,2,1),strides=[1,2,2,1],padding=\"VALID\",name=\"pool1\")\n",
    "\n",
    "#CONV LAYER_2\n",
    "conv2_weigts=tf.Variable(tf.truncated_normal([3,3,32,64],stddev=0.1),name=\"conv2_w\")\n",
    "conv2_bias=tf.Variable(tf.truncated_normal([64],0,0.1),name=\"conv2_b\")\n",
    "conv2=tf.add(tf.nn.conv2d(pool1,conv2_weigts,strides=[1,1,1,1],padding=\"SAME\",name=\"conv2\"),conv2_bias)\n",
    "conv2_bn=tf.contrib.layers.batch_norm(conv2,center=True,scale=True,is_training=phase)\n",
    "relu2=tf.nn.relu(conv2_bn,name=\"relu2\")\n",
    "pool2=tf.nn.max_pool(relu2,ksize=(1,2,2,1),strides=[1,2,2,1],padding=\"VALID\",name=\"pool2\")\n",
    "\n",
    "#FULL_CONNECTED LAYER_3\n",
    "reshape=tf.reshape(pool2,shape=[-1,pool2.shape[1]*pool2.shape[2]*pool2.shape[3]])\n",
    "fc1_weights=tf.Variable(tf.random_normal([int(reshape.shape[1]),512],0,0.5),name=\"fc1_w\")\n",
    "fc1_bias=tf.Variable(tf.random_normal([512],0,0.1),name=\"fc1_b\")\n",
    "\n",
    "fc1=tf.matmul(reshape,fc1_weights)+fc1_bias\n",
    "fc1=tf.nn.relu(fc1,name=\"fc1_relu\")\n",
    "\n",
    "if train:\n",
    "    fc1=tf.nn.dropout(fc1,0.9,seed=seed,name=\"fc1_dropout\")\n",
    "    \n",
    "#FULL_CONNECTED LAYER_4\n",
    "fc2_weights=tf.Variable(tf.random_normal([512,n_classes],0,0.5),name=\"fc2_w\")\n",
    "fc2_bias=tf.Variable(tf.random_normal([n_classes],0,0.1),name=\"fc2_b\")\n",
    "logits=tf.add(tf.matmul(fc1,fc2_weights),fc2_bias,name=\"logits\")\n",
    "y_proba=tf.nn.softmax(logits,name=\"y_proba\")\n",
    "    \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    loss1=tf.losses.sparse_softmax_cross_entropy(y,logits)\n",
    "    loss2=tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,logits=logits))\n",
    "    \n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        # Ensures that we execute the update_ops before performing the train_step\n",
    "        train_op=tf.train.AdamOptimizer().minimize(loss1)                                                                   \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct=tf.nn.in_top_k(logits,y,1)\n",
    "    accuracy=tf.reduce_mean(tf.cast(correct,dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"init_and_save\"):\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generate(data,labels,batch_size):\n",
    "    global i\n",
    "    batch_x=data[i:i+batch_size]\n",
    "    batch_y=labels[i:i+batch_size]\n",
    "    i+=batch_size\n",
    "    #print(\"index :\",int(i/batch_size))\n",
    "    return batch_x,batch_y\n",
    "\n",
    "def station_epoch(step,total_steps): \n",
    "    dot_number=int(step/total_steps*100/2)\n",
    "    #print(step,total_steps)\n",
    "    #print(dot_number)\n",
    "    sta=\".\"*dot_number\n",
    "    null=\" \"*(50-dot_number)\n",
    "    print(\"   \",sta+null,\"% {}\".format(int((step+1)/total_steps*100)),end='\\r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ---------------------------------------------------------- #\n",
      "    ................................................   % 100\n",
      "     Epoch : 0 | Accuracy : 0.3371 | Loss : 145.9312\n",
      "# ---------------------------------------------------------- #\n",
      "    ................................................   % 100\n",
      "     Epoch : 1 | Accuracy : 0.3737 | Loss : 124.9279\n",
      "# ---------------------------------------------------------- #\n",
      "    ................................................   % 100\n",
      "     Epoch : 2 | Accuracy : 0.3708 | Loss : 115.5912\n",
      "# ---------------------------------------------------------- #\n",
      "    ................................................   % 100\n",
      "     Epoch : 3 | Accuracy : 0.4722 | Loss : 86.6411\n",
      "# ---------------------------------------------------------- #\n",
      "    ................................................   % 100\n",
      "     Epoch : 4 | Accuracy : 0.4781 | Loss : 72.7551\n",
      "# ---------------------------------------------------------- #\n",
      "    ................................................   % 100\n",
      "     Epoch : 5 | Accuracy : 0.5228 | Loss : 63.1681\n",
      "# ---------------------------------------------------------- #\n",
      "    ................................................   % 100\n",
      "     Epoch : 6 | Accuracy : 0.5233 | Loss : 62.8750\n",
      "# ---------------------------------------------------------- #\n",
      "    ................................................   % 100\n",
      "     Epoch : 7 | Accuracy : 0.6095 | Loss : 42.2345\n",
      "# ---------------------------------------------------------- #\n",
      "    ................................................   % 100\n",
      "     Epoch : 8 | Accuracy : 0.6300 | Loss : 40.6451\n",
      "# ---------------------------------------------------------- #\n",
      "    ................................................   % 100\n",
      "     Epoch : 9 | Accuracy : 0.6408 | Loss : 33.6409\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(epochs):\n",
    "        i=0\n",
    "        print(\"# ---------------------------------------------------------- #\")\n",
    "        total_steps=int(validation_data.shape[0]/batch_size)\n",
    "        for step in range(total_steps):\n",
    "            #print(step)\n",
    "            batch_x,batch_y=batch_generate(validation_data,validation_labels,batch_size)\n",
    "            sess.run(train_op,feed_dict={x:batch_x,y:batch_y,phase:1})\n",
    "            station_epoch(step,total_steps)\n",
    "        \n",
    "        print(end=\"\\n\")\n",
    "        #print(\"Calculation Loss ...\")    \n",
    "        l,a=sess.run([loss1,accuracy],feed_dict={x:test_data,y:test_labels,phase:0})\n",
    "        print(\"    \",\"Epoch :\",epoch,\"| Accuracy : {:.4f} | Loss : {:.4f}\".format(a,l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
