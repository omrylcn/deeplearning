{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolution is a mathematical operation that combines information from two sources to\n",
    "produce a new set of information. Specifically, it applies a special matrix known as the\n",
    "kernel to the input tensor to produce a set of matrices known as the feature maps. The kernel\n",
    "can be applied to the input tensor using any of the popular algorithms.\n",
    "\n",
    "The result of applying the previously-mentioned convolution algorithm is the feature\n",
    "map which is the filtered version of the original tensor. For example, the feature map could\n",
    "have only the outlines filtered from the original image. Hence, the kernel is also known as\n",
    "the filter. For each kernel, you get a separate 2D feature map.\n",
    "\n",
    "Depending on which features you want the network to learn, you have to\n",
    "apply the appropriate filters to emphasize the required features. However,\n",
    "with CNN, the model can automatically learn which kernels work best in\n",
    "the convolution layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist=input_data.read_data_sets(\"data\")\n",
    "\n",
    "x_tr=mnist.train.images\n",
    "y_tr=mnist.train.labels\n",
    "x_ts=mnist.test.images\n",
    "y_ts=mnist.test.labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes=10\n",
    "n_width=28\n",
    "n_height=28\n",
    "n_depth=1\n",
    "n_input=n_width*n_height*n_depth\n",
    "\n",
    "learning_rate=0.001\n",
    "n_epochs=10\n",
    "batch_size=256\n",
    "n_batches=int(mnist.train.num_examples/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.placeholder(tf.float32,shape=[None,n_input])\n",
    "y=tf.placeholder(tf.int32,shape=[None])\n",
    "i_x=tf.reshape(x,[-1,28,28,1])\n",
    "\n",
    "with tf.name_scope(\"conv_layer1\"):\n",
    "    layer1_w=tf.Variable(tf.random_normal(shape=[4,4,n_depth,16],stddev=0.1),name=\"l1_w\")\n",
    "    layer1_b=tf.Variable(tf.random_normal(shape=[16],stddev=0.1),name=\"l1_b\")\n",
    "    layer1_conv=tf.nn.relu(tf.nn.conv2d(i_x,layer1_w,strides=[1,2,2,1],padding=\"SAME\")+layer1_b)\n",
    "    layer1_pool=tf.nn.max_pool(layer1_conv,ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"SAME\")\n",
    "\n",
    "with tf.name_scope(\"conv_layer2\"):\n",
    "    layer2_w=tf.Variable(tf.random_normal(shape=[4,4,16,32],stddev=0.1),name=\"l2_w\")\n",
    "    layer2_b=tf.Variable(tf.random_normal(shape=[32],stddev=0.1),name=\"l2_b\")\n",
    "    layer2_conv=tf.nn.relu(tf.nn.conv2d(layer1_pool,layer2_w,strides=[1,2,2,1],padding=\"SAME\")+layer2_b)\n",
    "    layer2_pool=tf.nn.max_pool(layer2_conv,ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"SAME\")\n",
    "    \n",
    "with tf.name_scope(\"fc_layer3\"):\n",
    "    h,w,d=layer2_pool.shape[1:]\n",
    "    layer3_w=tf.Variable(tf.random_normal(shape=[int(h*w*d),124],stddev=0.01),name=\"l3_w\")\n",
    "    layer3_b=tf.Variable(tf.random_normal(shape=[124],stddev=0.01),name=\"l3_b\")\n",
    "    layer3_fc=tf.nn.relu(tf.matmul(tf.reshape(layer2_pool,[-1,h*w*d]),layer3_w)+layer3_b)\n",
    "\n",
    "with tf.name_scope(\"logits_layer4\"):\n",
    "    layer4_w=tf.Variable(tf.random_normal(shape=[124,10],stddev=0.01),name=\"l4_w\")\n",
    "    layer4_b=tf.Variable(tf.random_normal(shape=[10],stddev=0.01),name=\"l4_b\")\n",
    "    logits=tf.matmul(layer3_fc,layer4_w)+layer4_b\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses=tf.losses.sparse_softmax_cross_entropy(labels=y,logits=logits)\n",
    "train_op=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct=tf.nn.in_top_k(logits,y,1)\n",
    "accuracy=tf.reduce_mean(tf.cast(correct,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 | Loss : 0.1070 | Accuracy : 2.3020\n",
      "Epoch : 0 | Loss : 0.8620 | Accuracy : 0.4111\n",
      "Epoch : 1 | Loss : 0.8990 | Accuracy : 0.3259\n",
      "Epoch : 1 | Loss : 0.9350 | Accuracy : 0.2065\n",
      "Epoch : 2 | Loss : 0.9390 | Accuracy : 0.1960\n",
      "Epoch : 2 | Loss : 0.9420 | Accuracy : 0.1643\n",
      "Epoch : 3 | Loss : 0.9520 | Accuracy : 0.1483\n",
      "Epoch : 3 | Loss : 0.9620 | Accuracy : 0.1215\n",
      "Epoch : 4 | Loss : 0.9570 | Accuracy : 0.1265\n",
      "Epoch : 4 | Loss : 0.9710 | Accuracy : 0.0966\n",
      "Epoch : 5 | Loss : 0.9620 | Accuracy : 0.1106\n",
      "Epoch : 5 | Loss : 0.9610 | Accuracy : 0.1000\n",
      "Epoch : 6 | Loss : 0.9630 | Accuracy : 0.0952\n",
      "Epoch : 6 | Loss : 0.9700 | Accuracy : 0.0958\n",
      "Epoch : 7 | Loss : 0.9620 | Accuracy : 0.0921\n",
      "Epoch : 7 | Loss : 0.9630 | Accuracy : 0.0851\n",
      "Epoch : 8 | Loss : 0.9730 | Accuracy : 0.0794\n",
      "Epoch : 8 | Loss : 0.9680 | Accuracy : 0.0851\n",
      "Epoch : 9 | Loss : 0.9710 | Accuracy : 0.0800\n",
      "Epoch : 9 | Loss : 0.9690 | Accuracy : 0.0829\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(n_epochs):\n",
    "        n_steps=int(mnist.train.num_examples/batch_size)\n",
    "        for step in range(n_steps):\n",
    "            x_batch,y_batch=mnist.train.next_batch(batch_size)\n",
    "           # print(x_batch,y_batch)\n",
    "            sess.run(train_op ,feed_dict={x:x_batch,y:y_batch})\n",
    "            sess.run(accuracy ,feed_dict={x:x_batch,y:y_batch})\n",
    "            if step%200==0:\n",
    "                v=sess.run([accuracy,losses],feed_dict={x:x_ts[:1000],y:y_ts[:1000]})\n",
    "                print(\"Epoch : {} | Loss : {:.4f} | Accuracy : {:.4f}\".format(epoch,v[0],v[1]))\n",
    "                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import time \n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        544       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 64)        32832     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              3212288   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 3,255,914\n",
      "Trainable params: 3,255,914\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(16,4,activation=\"relu\",input_shape=(28,28,1),padding=\"SAME\",strides=2\n",
    "                        kernel_initializer='random_normal',bias_initializer='random_normal'))\n",
    "\n",
    "\n",
    "model.add(layers.MaxPool2D(2,2,padding=\"SAME\"))\n",
    "\n",
    "model.add(layers.Conv2D(32,4,activation=\"relu\",input_shape=(28,28,1),padding=\"SAME\",\n",
    "                        kernel_initializer='random_normal',bias_initializer='random_normal'))\n",
    "\n",
    "model.add(layers.MaxPool2D(2,2,padding=\"SAME\"))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(124, kernel_initializer='random_normal',bias_initializer='random_normal',activation=\"relu\"))\n",
    "model.add(layers.Dense(10, kernel_initializer='random_normal',bias_initializer='random_normal',activation=\"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",optimizer=\"Adam\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "55000/55000 [==============================] - 171s 3ms/step - loss: 0.1248 - acc: 0.9610\n",
      "Epoch 2/10\n",
      "55000/55000 [==============================] - 160s 3ms/step - loss: 0.0360 - acc: 0.9883\n",
      "Epoch 3/10\n",
      "55000/55000 [==============================] - 161s 3ms/step - loss: 0.0231 - acc: 0.9927\n",
      "Epoch 4/10\n",
      "55000/55000 [==============================] - 163s 3ms/step - loss: 0.0176 - acc: 0.9947\n",
      "Epoch 5/10\n",
      "55000/55000 [==============================] - 163s 3ms/step - loss: 0.0134 - acc: 0.9957\n",
      "Epoch 6/10\n",
      "55000/55000 [==============================] - 165s 3ms/step - loss: 0.0106 - acc: 0.9967\n",
      "Epoch 7/10\n",
      "55000/55000 [==============================] - 171s 3ms/step - loss: 0.0088 - acc: 0.9972\n",
      "Epoch 8/10\n",
      "55000/55000 [==============================] - 175s 3ms/step - loss: 0.0081 - acc: 0.9975\n",
      "Epoch 9/10\n",
      "55000/55000 [==============================] - 176s 3ms/step - loss: 0.0067 - acc: 0.9978\n",
      "Epoch 10/10\n",
      "55000/55000 [==============================] - 168s 3ms/step - loss: 0.0053 - acc: 0.9983\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7044bb7b00>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard = TensorBoard(log_dir='logs/{}'.format(time.time()),histogram_freq=0,  \n",
    "          write_graph=True, write_images=True)\n",
    "model.fit(x_tr.reshape(-1,28,28,1),y_tr,batch_size=64,epochs=10,callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
